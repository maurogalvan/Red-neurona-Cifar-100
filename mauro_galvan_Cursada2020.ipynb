{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mauro galvan - Cursada2020.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US_V8aUitiYE"
      },
      "source": [
        "# Trabajo práctico\n",
        "Convinando las técnicas descriptas anteriormente, implemente una CNN para el problema [CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html). Objetivos en el conjunto de validación:\n",
        "* accuracy > 0.3.\n",
        "* top-5 accuracy > 0.6.\n",
        "\n",
        "**NOTA:** No es necesario que su solución utilice todas las técnicas descriptas.\n",
        "\n",
        "### Opcional\n",
        "Intente implementar otra solución utilizando módulos fire que se describen en la arquitectura [SqueezeNet](https://arxiv.org/abs/1602.07360).\n",
        "\n",
        "## Requerimientos:\n",
        "\n",
        "\n",
        "1.   El notebook debe correr linealmente.\n",
        "2.   Debe entregarse con las salidas ejecutadas.\n",
        "3.   Debe incluirse una pequeña explicación antes de cada arquitectura probada.\n",
        "4.   Debe incluirse una conclusión al final del trabajo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yiuP1Zbt0cc"
      },
      "source": [
        "Importamos todas las librerias que vamos a necesitar en el desarrollo del trabajo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYEYsus6N5n3"
      },
      "source": [
        "#Imports\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten, Dropout, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.models import Model \n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuWfhJcKuNCz"
      },
      "source": [
        "Se nos dio el codigo para levantar el Cifar-100 directamente desde tensorflorw.\n",
        "Normalizamos para que las imagenes se encuentren en un rango de 0 a 1, ya que esto es necesario para el gradiente desendente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnYlS6dstbVk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10278b94-58b2-43d1-e467-1555700ec92d"
      },
      "source": [
        "#Levanta el cifar-100\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
        " \n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "#Normalizacion: \n",
        "x_train = x_train/255\n",
        "x_test = x_test/255"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPZn1ya4u4zy"
      },
      "source": [
        "Comence armando una red con lo aprendido en clase, generando una red CNN de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK7HwMKj1JH9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5579fe44-0a2c-4ac0-9bd2-d21c071965fb"
      },
      "source": [
        "#MODELO 1\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten, MaxPooling2D\n",
        "from tensorflow.keras import models\n",
        "import tensorflow as tf\n",
        "\n",
        "modelo = models.Sequential()\n",
        "\n",
        "# Primera capa\n",
        "modelo.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', input_shape=(32,32,3)))\n",
        "modelo.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# Segunda capa\n",
        "modelo.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
        "modelo.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# Aplanar, Capas densas y salida.\n",
        "modelo.add(Flatten())\n",
        "modelo.add(Dense(128,activation='relu'))\n",
        "modelo.add(Dense(100,activation='softmax'))\n",
        "\n",
        "#modelo = Model(inputs=i, outputs=d)\n",
        "modelo.summary()\n",
        "modelo.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy','top_k_categorical_accuracy'])\n",
        "\n",
        "#batch_size 100 con 64, 64, 128 -> 0.3757 es mas estable tambien\n",
        "\n",
        "history = modelo.fit(x_train, y_train, epochs=30, batch_size=100, verbose=2, validation_data=(x_test, y_test))\n",
        "test_acc = modelo.evaluate(x_test,  y_test, verbose=2)\n",
        "\n",
        "plt.plot(history.history['val_categorical_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "print()\n",
        "print(modelo.evaluate(x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 30, 30, 64)        1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               295040    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               12900     \n",
            "=================================================================\n",
            "Total params: 346,660\n",
            "Trainable params: 346,660\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0027s vs `on_train_batch_end` time: 0.0092s). Check your callbacks.\n",
            "500/500 - 3s - loss: 3.9000 - categorical_accuracy: 0.1109 - top_k_categorical_accuracy: 0.2995 - val_loss: 3.3928 - val_categorical_accuracy: 0.1942 - val_top_k_categorical_accuracy: 0.4649\n",
            "Epoch 2/30\n",
            "500/500 - 3s - loss: 3.1385 - categorical_accuracy: 0.2394 - top_k_categorical_accuracy: 0.5214 - val_loss: 3.0085 - val_categorical_accuracy: 0.2626 - val_top_k_categorical_accuracy: 0.5596\n",
            "Epoch 3/30\n",
            "500/500 - 3s - loss: 2.8323 - categorical_accuracy: 0.2977 - top_k_categorical_accuracy: 0.5965 - val_loss: 2.8407 - val_categorical_accuracy: 0.2999 - val_top_k_categorical_accuracy: 0.5977\n",
            "Epoch 4/30\n",
            "500/500 - 3s - loss: 2.6298 - categorical_accuracy: 0.3399 - top_k_categorical_accuracy: 0.6434 - val_loss: 2.7182 - val_categorical_accuracy: 0.3263 - val_top_k_categorical_accuracy: 0.6251\n",
            "Epoch 5/30\n",
            "500/500 - 3s - loss: 2.4751 - categorical_accuracy: 0.3718 - top_k_categorical_accuracy: 0.6771 - val_loss: 2.6400 - val_categorical_accuracy: 0.3360 - val_top_k_categorical_accuracy: 0.6422\n",
            "Epoch 6/30\n",
            "500/500 - 3s - loss: 2.3486 - categorical_accuracy: 0.3981 - top_k_categorical_accuracy: 0.7014 - val_loss: 2.5476 - val_categorical_accuracy: 0.3580 - val_top_k_categorical_accuracy: 0.6605\n",
            "Epoch 7/30\n",
            "500/500 - 3s - loss: 2.2408 - categorical_accuracy: 0.4211 - top_k_categorical_accuracy: 0.7246 - val_loss: 2.5263 - val_categorical_accuracy: 0.3725 - val_top_k_categorical_accuracy: 0.6647\n",
            "Epoch 8/30\n",
            "500/500 - 3s - loss: 2.1588 - categorical_accuracy: 0.4376 - top_k_categorical_accuracy: 0.7411 - val_loss: 2.5328 - val_categorical_accuracy: 0.3665 - val_top_k_categorical_accuracy: 0.6648\n",
            "Epoch 9/30\n",
            "500/500 - 3s - loss: 2.0723 - categorical_accuracy: 0.4596 - top_k_categorical_accuracy: 0.7569 - val_loss: 2.5390 - val_categorical_accuracy: 0.3701 - val_top_k_categorical_accuracy: 0.6667\n",
            "Epoch 10/30\n",
            "500/500 - 3s - loss: 2.0024 - categorical_accuracy: 0.4725 - top_k_categorical_accuracy: 0.7707 - val_loss: 2.5266 - val_categorical_accuracy: 0.3790 - val_top_k_categorical_accuracy: 0.6793\n",
            "Epoch 11/30\n",
            "500/500 - 3s - loss: 1.9342 - categorical_accuracy: 0.4884 - top_k_categorical_accuracy: 0.7837 - val_loss: 2.4801 - val_categorical_accuracy: 0.3849 - val_top_k_categorical_accuracy: 0.6789\n",
            "Epoch 12/30\n",
            "500/500 - 3s - loss: 1.8727 - categorical_accuracy: 0.4996 - top_k_categorical_accuracy: 0.7923 - val_loss: 2.4995 - val_categorical_accuracy: 0.3871 - val_top_k_categorical_accuracy: 0.6833\n",
            "Epoch 13/30\n",
            "500/500 - 3s - loss: 1.8135 - categorical_accuracy: 0.5156 - top_k_categorical_accuracy: 0.8035 - val_loss: 2.4846 - val_categorical_accuracy: 0.3920 - val_top_k_categorical_accuracy: 0.6826\n",
            "Epoch 14/30\n",
            "500/500 - 3s - loss: 1.7642 - categorical_accuracy: 0.5252 - top_k_categorical_accuracy: 0.8120 - val_loss: 2.5654 - val_categorical_accuracy: 0.3832 - val_top_k_categorical_accuracy: 0.6792\n",
            "Epoch 15/30\n",
            "500/500 - 3s - loss: 1.7023 - categorical_accuracy: 0.5416 - top_k_categorical_accuracy: 0.8241 - val_loss: 2.5709 - val_categorical_accuracy: 0.3869 - val_top_k_categorical_accuracy: 0.6757\n",
            "Epoch 16/30\n",
            "500/500 - 3s - loss: 1.6586 - categorical_accuracy: 0.5513 - top_k_categorical_accuracy: 0.8311 - val_loss: 2.6065 - val_categorical_accuracy: 0.3817 - val_top_k_categorical_accuracy: 0.6776\n",
            "Epoch 17/30\n",
            "500/500 - 3s - loss: 1.6068 - categorical_accuracy: 0.5632 - top_k_categorical_accuracy: 0.8388 - val_loss: 2.6231 - val_categorical_accuracy: 0.3806 - val_top_k_categorical_accuracy: 0.6780\n",
            "Epoch 18/30\n",
            "500/500 - 3s - loss: 1.5621 - categorical_accuracy: 0.5730 - top_k_categorical_accuracy: 0.8470 - val_loss: 2.6857 - val_categorical_accuracy: 0.3816 - val_top_k_categorical_accuracy: 0.6733\n",
            "Epoch 19/30\n",
            "500/500 - 3s - loss: 1.5138 - categorical_accuracy: 0.5839 - top_k_categorical_accuracy: 0.8538 - val_loss: 2.8265 - val_categorical_accuracy: 0.3754 - val_top_k_categorical_accuracy: 0.6627\n",
            "Epoch 20/30\n",
            "500/500 - 3s - loss: 1.4682 - categorical_accuracy: 0.5932 - top_k_categorical_accuracy: 0.8621 - val_loss: 2.7303 - val_categorical_accuracy: 0.3812 - val_top_k_categorical_accuracy: 0.6708\n",
            "Epoch 21/30\n",
            "500/500 - 3s - loss: 1.4331 - categorical_accuracy: 0.6044 - top_k_categorical_accuracy: 0.8679 - val_loss: 2.7466 - val_categorical_accuracy: 0.3835 - val_top_k_categorical_accuracy: 0.6700\n",
            "Epoch 22/30\n",
            "500/500 - 3s - loss: 1.3958 - categorical_accuracy: 0.6097 - top_k_categorical_accuracy: 0.8736 - val_loss: 2.8108 - val_categorical_accuracy: 0.3800 - val_top_k_categorical_accuracy: 0.6672\n",
            "Epoch 23/30\n",
            "500/500 - 3s - loss: 1.3484 - categorical_accuracy: 0.6237 - top_k_categorical_accuracy: 0.8817 - val_loss: 2.8517 - val_categorical_accuracy: 0.3710 - val_top_k_categorical_accuracy: 0.6593\n",
            "Epoch 24/30\n",
            "500/500 - 3s - loss: 1.3196 - categorical_accuracy: 0.6308 - top_k_categorical_accuracy: 0.8848 - val_loss: 2.8988 - val_categorical_accuracy: 0.3713 - val_top_k_categorical_accuracy: 0.6654\n",
            "Epoch 25/30\n",
            "500/500 - 3s - loss: 1.2826 - categorical_accuracy: 0.6400 - top_k_categorical_accuracy: 0.8905 - val_loss: 2.9115 - val_categorical_accuracy: 0.3726 - val_top_k_categorical_accuracy: 0.6656\n",
            "Epoch 26/30\n",
            "500/500 - 3s - loss: 1.2485 - categorical_accuracy: 0.6471 - top_k_categorical_accuracy: 0.8950 - val_loss: 3.0528 - val_categorical_accuracy: 0.3624 - val_top_k_categorical_accuracy: 0.6570\n",
            "Epoch 27/30\n",
            "500/500 - 3s - loss: 1.2123 - categorical_accuracy: 0.6552 - top_k_categorical_accuracy: 0.9005 - val_loss: 3.0914 - val_categorical_accuracy: 0.3781 - val_top_k_categorical_accuracy: 0.6611\n",
            "Epoch 28/30\n",
            "500/500 - 3s - loss: 1.1815 - categorical_accuracy: 0.6644 - top_k_categorical_accuracy: 0.9047 - val_loss: 3.2017 - val_categorical_accuracy: 0.3606 - val_top_k_categorical_accuracy: 0.6531\n",
            "Epoch 29/30\n",
            "500/500 - 3s - loss: 1.1513 - categorical_accuracy: 0.6725 - top_k_categorical_accuracy: 0.9088 - val_loss: 3.2154 - val_categorical_accuracy: 0.3622 - val_top_k_categorical_accuracy: 0.6560\n",
            "Epoch 30/30\n",
            "500/500 - 3s - loss: 1.1201 - categorical_accuracy: 0.6777 - top_k_categorical_accuracy: 0.9132 - val_loss: 3.2676 - val_categorical_accuracy: 0.3568 - val_top_k_categorical_accuracy: 0.6500\n",
            "313/313 - 1s - loss: 3.2676 - categorical_accuracy: 0.3568 - top_k_categorical_accuracy: 0.6500\n",
            "\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 3.2676 - categorical_accuracy: 0.3568 - top_k_categorical_accuracy: 0.6500\n",
            "[3.2676477432250977, 0.35679998993873596, 0.6499999761581421]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzewTL8N138J"
      },
      "source": [
        "La red cuenta con en primera instancia un modelo secuencial ya que vamos a armar una red CNN. En la primera capa le agrege **Conv2D** con 64 filtros entonces en una imagen de entrada de 32x32x3 y un kernel de 3x3 se estiraría cada campo en un vector de largo 27, y se ejecutarían 32 x 32 =1024 convoluciones, y le aplicamos la funcion de activacion 'Relu' que es la más común en las redes neuronales convolucionales que básicamente impide que valores negativos se propaguen por la red, ademas se agrego **Maxpooling** ya que usa para requerir un 75% menos de parámetros y propaga solo las mejores características. Lo explicado anterior se agrego dos veces.\n",
        "Despues llegamos a la etapa de 'aplanado' la cual es **Flatten** convierte los elementos de la matriz de imagenes de entrada en un array plano; agregamos una **capa densa** de 128 filtros con una funcion de activacion 'Relu' y como ultima capa, se agrego una **densa** de 100 que son las clases del Cifar-100 con una funcion de activacion 'Softmax' para transformar las salidas a una representación en forma de probabilidades, de tal manera que el sumatorio de todas las probabilidades de las salidas, esta acotada entre 0 y 1.\n",
        "\n",
        "La red presentada aca arriba fue la primera que hice, el modelo alcanza una precisión de 67.77% para el set de entrenamiento y del 35.68% para el set de validación. \n",
        "Particularmente no me sentia satisfecho con el resultado, ya que se ve claramente que en set de validacion cae con el tiempo. Por ende me di la tarea de tratar de mejorarla."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_lq8I_v7vjO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a1083591-61ca-4e63-90f1-5bd21660b6ce"
      },
      "source": [
        "#MODELO 2\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten, MaxPooling2D, Dropout\n",
        "from tensorflow.keras import models\n",
        "import tensorflow as tf\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "modelo1 = models.Sequential()\n",
        "\n",
        "# Añadimos la primera capa\n",
        "modelo1.add(Conv2D(64,(3,3), activation = 'relu', input_shape = (32,32,3)))\n",
        "modelo1.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "# Añadimos la segunda capa\n",
        "modelo1.add(Conv2D(64,(3,3), activation = 'relu'))\n",
        "modelo1.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "# Hacemos un flatten para poder usar una red fully connected\n",
        "modelo1.add(Flatten())\n",
        "modelo1.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Añadimos una capa softmax para que podamos clasificar las imágenes\n",
        "modelo1.add(Dense(100, activation='softmax'))\n",
        "\n",
        "\n",
        "modelo1.summary()\n",
        "modelo1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy','top_k_categorical_accuracy'])\n",
        "\n",
        "\n",
        "history = modelo1.fit(x_train, y_train, epochs=20, batch_size=200, verbose=1, validation_data=(x_test, y_test))\n",
        "test_acc = modelo1.evaluate(x_test,  y_test, verbose=2)\n",
        "\n",
        "plt.plot(history.history['val_categorical_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "print()\n",
        "print(modelo1.evaluate(x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 30, 30, 64)        1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 64)                147520    \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 100)               6500      \n",
            "=================================================================\n",
            "Total params: 192,740\n",
            "Trainable params: 192,740\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 4.1558 - categorical_accuracy: 0.0715 - top_k_categorical_accuracy: 0.2176 - val_loss: 3.7595 - val_categorical_accuracy: 0.1327 - val_top_k_categorical_accuracy: 0.3663\n",
            "Epoch 2/20\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 3.5195 - categorical_accuracy: 0.1726 - top_k_categorical_accuracy: 0.4239 - val_loss: 3.4089 - val_categorical_accuracy: 0.1931 - val_top_k_categorical_accuracy: 0.4573\n",
            "Epoch 3/20\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 3.2330 - categorical_accuracy: 0.2236 - top_k_categorical_accuracy: 0.5009 - val_loss: 3.1817 - val_categorical_accuracy: 0.2352 - val_top_k_categorical_accuracy: 0.5168\n",
            "Epoch 4/20\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 3.0565 - categorical_accuracy: 0.2553 - top_k_categorical_accuracy: 0.5449 - val_loss: 3.0389 - val_categorical_accuracy: 0.2613 - val_top_k_categorical_accuracy: 0.5460\n",
            "Epoch 5/20\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 2.9128 - categorical_accuracy: 0.2843 - top_k_categorical_accuracy: 0.5790 - val_loss: 2.9423 - val_categorical_accuracy: 0.2858 - val_top_k_categorical_accuracy: 0.5737\n",
            "Epoch 6/20\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 2.7976 - categorical_accuracy: 0.3079 - top_k_categorical_accuracy: 0.6060 - val_loss: 2.8559 - val_categorical_accuracy: 0.2987 - val_top_k_categorical_accuracy: 0.5965\n",
            "Epoch 7/20\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 2.6993 - categorical_accuracy: 0.3257 - top_k_categorical_accuracy: 0.6296 - val_loss: 2.8118 - val_categorical_accuracy: 0.3062 - val_top_k_categorical_accuracy: 0.6016\n",
            "Epoch 8/20\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 2.6089 - categorical_accuracy: 0.3452 - top_k_categorical_accuracy: 0.6502 - val_loss: 2.7443 - val_categorical_accuracy: 0.3251 - val_top_k_categorical_accuracy: 0.6219\n",
            "Epoch 9/20\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 2.5367 - categorical_accuracy: 0.3618 - top_k_categorical_accuracy: 0.6653 - val_loss: 2.7505 - val_categorical_accuracy: 0.3213 - val_top_k_categorical_accuracy: 0.6174\n",
            "Epoch 10/20\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 2.4728 - categorical_accuracy: 0.3710 - top_k_categorical_accuracy: 0.6794 - val_loss: 2.6878 - val_categorical_accuracy: 0.3365 - val_top_k_categorical_accuracy: 0.6355\n",
            "Epoch 11/20\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 2.4214 - categorical_accuracy: 0.3830 - top_k_categorical_accuracy: 0.6889 - val_loss: 2.6123 - val_categorical_accuracy: 0.3561 - val_top_k_categorical_accuracy: 0.6470\n",
            "Epoch 12/20\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 2.3678 - categorical_accuracy: 0.3941 - top_k_categorical_accuracy: 0.6998 - val_loss: 2.5715 - val_categorical_accuracy: 0.3612 - val_top_k_categorical_accuracy: 0.6631\n",
            "Epoch 13/20\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 2.3172 - categorical_accuracy: 0.4030 - top_k_categorical_accuracy: 0.7124 - val_loss: 2.5835 - val_categorical_accuracy: 0.3607 - val_top_k_categorical_accuracy: 0.6613\n",
            "Epoch 14/20\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 2.2750 - categorical_accuracy: 0.4135 - top_k_categorical_accuracy: 0.7187 - val_loss: 2.5438 - val_categorical_accuracy: 0.3656 - val_top_k_categorical_accuracy: 0.6669\n",
            "Epoch 15/20\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 2.2404 - categorical_accuracy: 0.4192 - top_k_categorical_accuracy: 0.7263 - val_loss: 2.5322 - val_categorical_accuracy: 0.3678 - val_top_k_categorical_accuracy: 0.6699\n",
            "Epoch 16/20\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 2.2056 - categorical_accuracy: 0.4272 - top_k_categorical_accuracy: 0.7330 - val_loss: 2.5107 - val_categorical_accuracy: 0.3735 - val_top_k_categorical_accuracy: 0.6739\n",
            "Epoch 17/20\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 2.1709 - categorical_accuracy: 0.4371 - top_k_categorical_accuracy: 0.7394 - val_loss: 2.4958 - val_categorical_accuracy: 0.3803 - val_top_k_categorical_accuracy: 0.6784\n",
            "Epoch 18/20\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 2.1418 - categorical_accuracy: 0.4413 - top_k_categorical_accuracy: 0.7464 - val_loss: 2.4845 - val_categorical_accuracy: 0.3785 - val_top_k_categorical_accuracy: 0.6816\n",
            "Epoch 19/20\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 2.1114 - categorical_accuracy: 0.4459 - top_k_categorical_accuracy: 0.7504 - val_loss: 2.4898 - val_categorical_accuracy: 0.3819 - val_top_k_categorical_accuracy: 0.6782\n",
            "Epoch 20/20\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 2.0867 - categorical_accuracy: 0.4523 - top_k_categorical_accuracy: 0.7568 - val_loss: 2.5321 - val_categorical_accuracy: 0.3710 - val_top_k_categorical_accuracy: 0.6710\n",
            "313/313 - 1s - loss: 2.5321 - categorical_accuracy: 0.3710 - top_k_categorical_accuracy: 0.6710\n",
            "\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.5321 - categorical_accuracy: 0.3710 - top_k_categorical_accuracy: 0.6710\n",
            "[2.5320863723754883, 0.3709999918937683, 0.6710000038146973]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaVIysxLLfEg"
      },
      "source": [
        "En esta red de arriba vemos que es muy parecida pero al cambiar algunos parametros note que mejoro un poco en en el valor de verificacion pero empeoro muchisimo el de entrenamiento, consiguiendo:\n",
        "una precisión de 45.23% para el set de entrenamiento y del 37.10% para el set de validación. \n",
        "Tampoco me quede satisfecho con esta solucion, por ende segui buscando."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8D7FeFOMi5m"
      },
      "source": [
        "Me di la tarea de buscar redes neuronales que tengan que ver con imagenes, tratando de conseguir alguna que haga algo parecido a lo que yo estoy buscando.\n",
        "Entonces en mi busqueda encontre dos que me interesaron bastante y trate de amoldar esa red a la mia.\n",
        "* Red neuronal sobre deteccion de objetos, en esta red detectaba muñecos de legos en vivo. Info: https://www.aprendemachinelearning.com/deteccion-de-objetos-con-python-yolo-keras-tutorial/\n",
        "* Red neuronal sobre reconocimientos de imagenes. Info: https://towardsdatascience.com/image-recognition-with-machine-learning-on-python-convolutional-neural-network-363073020588\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UInvBt1YQgCP"
      },
      "source": [
        "Entonces arme la siguiente red mucho mas grande que las anteriores que tiene en la primera capa, dos **Conv2D** con 128 filtros los dos y la funcion de activacion es la 'Relu' igual que la anterior red, en este le agregamos el padding 'same', ademas agregamos **AveragePooling2D** que en la anterior red usamos Maxpooling, con el Average conseguimos que se seleccione el valor medio de todos los píxeles del lote en ves de el valor de píxel máximo del lote, tambien agregamos un **dropout** que por cada nueva entrada a la red en la fase de entrenamiento, se desactivará aleatoriamente un porcentaje de las neuronas en cada capa oculta, acorde a una probabilidad de descarte previamente definida, esto ayude a que la red no \"memorice\". La segunda y tercera capa son iguales, solo que fui alterando los valores para tratar de conseguir un valor mas alto de aprendizaje y de valoracion. En la cuarta capa aplanamos y agregamos una capa densa con 1024 filtros con la funcion de activacion 'Relu', un dropout y una capa densa con 'Softmax' para el final."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGUG75x6ODGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b7eae94-a04f-4010-8b1e-f75a545bb9e5"
      },
      "source": [
        "#MODELO 3\n",
        "\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU\n",
        "\n",
        "i = Input(shape=(x_train.shape[1],x_train.shape[2],3))\n",
        "\n",
        "#Añadimos la primera capa\n",
        "d = Conv2D(128,(3,3),activation='relu', padding='same')(i)\n",
        "d = Conv2D(128,(3,3),activation='relu', padding='same')(d)\n",
        "d = AveragePooling2D(pool_size=(2, 2))(d)\n",
        "d = Dropout(0.2)(d)\n",
        "\n",
        "#Añadimos la segunda capa\n",
        "d = Conv2D(256, (3,3), activation='relu', padding='same')(d)\n",
        "d = Conv2D(256, (3,3), activation='relu', padding='same')(d)\n",
        "d = AveragePooling2D(pool_size=(2, 2))(d)\n",
        "d = Dropout(0.25)(d)\n",
        "\n",
        "#Añadimos la tercera capa\n",
        "d = Conv2D(512,(3,3),activation='relu', padding='same')(d)\n",
        "d = Conv2D(512,(3,3),activation='relu', padding='same')(d)\n",
        "d = AveragePooling2D(pool_size=(2, 2))(d)\n",
        "d = Dropout(0.5)(d)\n",
        "\n",
        "#Aplanamos, y agregamos una capa densa, un dropout y la capa densa de salida.\n",
        "d = Flatten()(d)\n",
        "d = Dense(1024, activation='relu')(d)\n",
        "d = Dropout(0.5)(d)\n",
        "d = Dense(100, activation='softmax')(d)\n",
        "\n",
        "model = Model(inputs=i, outputs=d)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy','top_k_categorical_accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=80, batch_size=650, verbose=1, validation_data=(x_test,y_test))\n",
        "\n",
        "print()\n",
        "print(model.evaluate(x_test,y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_33\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_17 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_92 (Conv2D)           (None, 32, 32, 128)       3584      \n",
            "_________________________________________________________________\n",
            "conv2d_93 (Conv2D)           (None, 32, 32, 128)       147584    \n",
            "_________________________________________________________________\n",
            "average_pooling2d_51 (Averag (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_46 (Dropout)         (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_94 (Conv2D)           (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "conv2d_95 (Conv2D)           (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "average_pooling2d_52 (Averag (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_47 (Dropout)         (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_96 (Conv2D)           (None, 8, 8, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_97 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "average_pooling2d_53 (Averag (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_48 (Dropout)         (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_23 (Flatten)         (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 1024)              8389632   \n",
            "_________________________________________________________________\n",
            "dropout_49 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 100)               102500    \n",
            "=================================================================\n",
            "Total params: 13,068,516\n",
            "Trainable params: 13,068,516\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/80\n",
            " 2/77 [..............................] - ETA: 18s - loss: 4.6147 - categorical_accuracy: 0.0162 - top_k_categorical_accuracy: 0.0615WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0922s vs `on_train_batch_end` time: 0.1849s). Check your callbacks.\n",
            "77/77 [==============================] - 22s 291ms/step - loss: 4.5416 - categorical_accuracy: 0.0166 - top_k_categorical_accuracy: 0.0755 - val_loss: 4.2981 - val_categorical_accuracy: 0.0356 - val_top_k_categorical_accuracy: 0.1526\n",
            "Epoch 2/80\n",
            "77/77 [==============================] - 23s 294ms/step - loss: 4.0986 - categorical_accuracy: 0.0639 - top_k_categorical_accuracy: 0.2262 - val_loss: 3.7362 - val_categorical_accuracy: 0.1227 - val_top_k_categorical_accuracy: 0.3447\n",
            "Epoch 3/80\n",
            "77/77 [==============================] - 22s 289ms/step - loss: 3.6949 - categorical_accuracy: 0.1307 - top_k_categorical_accuracy: 0.3631 - val_loss: 3.4099 - val_categorical_accuracy: 0.1817 - val_top_k_categorical_accuracy: 0.4447\n",
            "Epoch 4/80\n",
            "77/77 [==============================] - 22s 288ms/step - loss: 3.3835 - categorical_accuracy: 0.1868 - top_k_categorical_accuracy: 0.4528 - val_loss: 3.1452 - val_categorical_accuracy: 0.2458 - val_top_k_categorical_accuracy: 0.5285\n",
            "Epoch 5/80\n",
            "77/77 [==============================] - 22s 291ms/step - loss: 3.1269 - categorical_accuracy: 0.2339 - top_k_categorical_accuracy: 0.5231 - val_loss: 2.8789 - val_categorical_accuracy: 0.2848 - val_top_k_categorical_accuracy: 0.5851\n",
            "Epoch 6/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 2.9204 - categorical_accuracy: 0.2746 - top_k_categorical_accuracy: 0.5727 - val_loss: 2.7592 - val_categorical_accuracy: 0.3189 - val_top_k_categorical_accuracy: 0.6107\n",
            "Epoch 7/80\n",
            "77/77 [==============================] - 22s 291ms/step - loss: 2.7521 - categorical_accuracy: 0.3083 - top_k_categorical_accuracy: 0.6113 - val_loss: 2.5788 - val_categorical_accuracy: 0.3516 - val_top_k_categorical_accuracy: 0.6544\n",
            "Epoch 8/80\n",
            "77/77 [==============================] - 22s 291ms/step - loss: 2.5926 - categorical_accuracy: 0.3401 - top_k_categorical_accuracy: 0.6508 - val_loss: 2.4379 - val_categorical_accuracy: 0.3817 - val_top_k_categorical_accuracy: 0.6814\n",
            "Epoch 9/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 2.4438 - categorical_accuracy: 0.3701 - top_k_categorical_accuracy: 0.6822 - val_loss: 2.3140 - val_categorical_accuracy: 0.4037 - val_top_k_categorical_accuracy: 0.7054\n",
            "Epoch 10/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 2.3280 - categorical_accuracy: 0.3926 - top_k_categorical_accuracy: 0.7088 - val_loss: 2.2197 - val_categorical_accuracy: 0.4182 - val_top_k_categorical_accuracy: 0.7242\n",
            "Epoch 11/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 2.1998 - categorical_accuracy: 0.4188 - top_k_categorical_accuracy: 0.7324 - val_loss: 2.1637 - val_categorical_accuracy: 0.4380 - val_top_k_categorical_accuracy: 0.7383\n",
            "Epoch 12/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 2.1063 - categorical_accuracy: 0.4387 - top_k_categorical_accuracy: 0.7525 - val_loss: 2.0904 - val_categorical_accuracy: 0.4513 - val_top_k_categorical_accuracy: 0.7550\n",
            "Epoch 13/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 2.0029 - categorical_accuracy: 0.4607 - top_k_categorical_accuracy: 0.7742 - val_loss: 2.0226 - val_categorical_accuracy: 0.4698 - val_top_k_categorical_accuracy: 0.7658\n",
            "Epoch 14/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 1.9158 - categorical_accuracy: 0.4821 - top_k_categorical_accuracy: 0.7895 - val_loss: 1.9857 - val_categorical_accuracy: 0.4757 - val_top_k_categorical_accuracy: 0.7727\n",
            "Epoch 15/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 1.8513 - categorical_accuracy: 0.4952 - top_k_categorical_accuracy: 0.8022 - val_loss: 1.9640 - val_categorical_accuracy: 0.4845 - val_top_k_categorical_accuracy: 0.7757\n",
            "Epoch 16/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 1.7633 - categorical_accuracy: 0.5138 - top_k_categorical_accuracy: 0.8182 - val_loss: 1.8958 - val_categorical_accuracy: 0.4947 - val_top_k_categorical_accuracy: 0.7884\n",
            "Epoch 17/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 1.6878 - categorical_accuracy: 0.5317 - top_k_categorical_accuracy: 0.8318 - val_loss: 1.8681 - val_categorical_accuracy: 0.5020 - val_top_k_categorical_accuracy: 0.7911\n",
            "Epoch 18/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 1.6238 - categorical_accuracy: 0.5457 - top_k_categorical_accuracy: 0.8419 - val_loss: 1.8362 - val_categorical_accuracy: 0.5126 - val_top_k_categorical_accuracy: 0.7973\n",
            "Epoch 19/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 1.5671 - categorical_accuracy: 0.5565 - top_k_categorical_accuracy: 0.8519 - val_loss: 1.8070 - val_categorical_accuracy: 0.5166 - val_top_k_categorical_accuracy: 0.7991\n",
            "Epoch 20/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 1.5067 - categorical_accuracy: 0.5747 - top_k_categorical_accuracy: 0.8634 - val_loss: 1.7992 - val_categorical_accuracy: 0.5168 - val_top_k_categorical_accuracy: 0.8004\n",
            "Epoch 21/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 1.4430 - categorical_accuracy: 0.5877 - top_k_categorical_accuracy: 0.8739 - val_loss: 1.7873 - val_categorical_accuracy: 0.5252 - val_top_k_categorical_accuracy: 0.8044\n",
            "Epoch 22/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 1.3965 - categorical_accuracy: 0.5957 - top_k_categorical_accuracy: 0.8809 - val_loss: 1.7760 - val_categorical_accuracy: 0.5280 - val_top_k_categorical_accuracy: 0.8092\n",
            "Epoch 23/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 1.3412 - categorical_accuracy: 0.6124 - top_k_categorical_accuracy: 0.8892 - val_loss: 1.7743 - val_categorical_accuracy: 0.5306 - val_top_k_categorical_accuracy: 0.8101\n",
            "Epoch 24/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 1.2872 - categorical_accuracy: 0.6254 - top_k_categorical_accuracy: 0.8973 - val_loss: 1.7537 - val_categorical_accuracy: 0.5364 - val_top_k_categorical_accuracy: 0.8154\n",
            "Epoch 25/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 1.2609 - categorical_accuracy: 0.6297 - top_k_categorical_accuracy: 0.9037 - val_loss: 1.7356 - val_categorical_accuracy: 0.5373 - val_top_k_categorical_accuracy: 0.8162\n",
            "Epoch 26/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 1.2118 - categorical_accuracy: 0.6453 - top_k_categorical_accuracy: 0.9097 - val_loss: 1.7229 - val_categorical_accuracy: 0.5444 - val_top_k_categorical_accuracy: 0.8182\n",
            "Epoch 27/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 1.1735 - categorical_accuracy: 0.6544 - top_k_categorical_accuracy: 0.9156 - val_loss: 1.7508 - val_categorical_accuracy: 0.5417 - val_top_k_categorical_accuracy: 0.8143\n",
            "Epoch 28/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 1.1463 - categorical_accuracy: 0.6582 - top_k_categorical_accuracy: 0.9183 - val_loss: 1.7215 - val_categorical_accuracy: 0.5465 - val_top_k_categorical_accuracy: 0.8181\n",
            "Epoch 29/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 1.1030 - categorical_accuracy: 0.6729 - top_k_categorical_accuracy: 0.9232 - val_loss: 1.7761 - val_categorical_accuracy: 0.5422 - val_top_k_categorical_accuracy: 0.8169\n",
            "Epoch 30/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 1.0652 - categorical_accuracy: 0.6826 - top_k_categorical_accuracy: 0.9291 - val_loss: 1.7267 - val_categorical_accuracy: 0.5517 - val_top_k_categorical_accuracy: 0.8194\n",
            "Epoch 31/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 1.0210 - categorical_accuracy: 0.6916 - top_k_categorical_accuracy: 0.9356 - val_loss: 1.7415 - val_categorical_accuracy: 0.5547 - val_top_k_categorical_accuracy: 0.8239\n",
            "Epoch 32/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 0.9998 - categorical_accuracy: 0.6988 - top_k_categorical_accuracy: 0.9379 - val_loss: 1.7230 - val_categorical_accuracy: 0.5517 - val_top_k_categorical_accuracy: 0.8205\n",
            "Epoch 33/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 0.9822 - categorical_accuracy: 0.7040 - top_k_categorical_accuracy: 0.9396 - val_loss: 1.7406 - val_categorical_accuracy: 0.5512 - val_top_k_categorical_accuracy: 0.8222\n",
            "Epoch 34/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.9324 - categorical_accuracy: 0.7155 - top_k_categorical_accuracy: 0.9458 - val_loss: 1.7303 - val_categorical_accuracy: 0.5546 - val_top_k_categorical_accuracy: 0.8208\n",
            "Epoch 35/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 0.9112 - categorical_accuracy: 0.7225 - top_k_categorical_accuracy: 0.9479 - val_loss: 1.7226 - val_categorical_accuracy: 0.5532 - val_top_k_categorical_accuracy: 0.8249\n",
            "Epoch 36/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.8824 - categorical_accuracy: 0.7299 - top_k_categorical_accuracy: 0.9512 - val_loss: 1.7229 - val_categorical_accuracy: 0.5555 - val_top_k_categorical_accuracy: 0.8271\n",
            "Epoch 37/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.8624 - categorical_accuracy: 0.7350 - top_k_categorical_accuracy: 0.9539 - val_loss: 1.7289 - val_categorical_accuracy: 0.5569 - val_top_k_categorical_accuracy: 0.8246\n",
            "Epoch 38/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 0.8494 - categorical_accuracy: 0.7398 - top_k_categorical_accuracy: 0.9544 - val_loss: 1.7535 - val_categorical_accuracy: 0.5580 - val_top_k_categorical_accuracy: 0.8252\n",
            "Epoch 39/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 0.8135 - categorical_accuracy: 0.7489 - top_k_categorical_accuracy: 0.9595 - val_loss: 1.7545 - val_categorical_accuracy: 0.5591 - val_top_k_categorical_accuracy: 0.8261\n",
            "Epoch 40/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 0.7980 - categorical_accuracy: 0.7546 - top_k_categorical_accuracy: 0.9614 - val_loss: 1.7181 - val_categorical_accuracy: 0.5622 - val_top_k_categorical_accuracy: 0.8271\n",
            "Epoch 41/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.7877 - categorical_accuracy: 0.7557 - top_k_categorical_accuracy: 0.9617 - val_loss: 1.7517 - val_categorical_accuracy: 0.5591 - val_top_k_categorical_accuracy: 0.8257\n",
            "Epoch 42/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.7612 - categorical_accuracy: 0.7650 - top_k_categorical_accuracy: 0.9635 - val_loss: 1.7611 - val_categorical_accuracy: 0.5674 - val_top_k_categorical_accuracy: 0.8296\n",
            "Epoch 43/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.7385 - categorical_accuracy: 0.7711 - top_k_categorical_accuracy: 0.9665 - val_loss: 1.8015 - val_categorical_accuracy: 0.5616 - val_top_k_categorical_accuracy: 0.8233\n",
            "Epoch 44/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 0.7213 - categorical_accuracy: 0.7746 - top_k_categorical_accuracy: 0.9688 - val_loss: 1.7662 - val_categorical_accuracy: 0.5624 - val_top_k_categorical_accuracy: 0.8240\n",
            "Epoch 45/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 0.7011 - categorical_accuracy: 0.7822 - top_k_categorical_accuracy: 0.9697 - val_loss: 1.7653 - val_categorical_accuracy: 0.5606 - val_top_k_categorical_accuracy: 0.8270\n",
            "Epoch 46/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 0.6946 - categorical_accuracy: 0.7835 - top_k_categorical_accuracy: 0.9702 - val_loss: 1.7800 - val_categorical_accuracy: 0.5611 - val_top_k_categorical_accuracy: 0.8255\n",
            "Epoch 47/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 0.6768 - categorical_accuracy: 0.7865 - top_k_categorical_accuracy: 0.9713 - val_loss: 1.7396 - val_categorical_accuracy: 0.5719 - val_top_k_categorical_accuracy: 0.8284\n",
            "Epoch 48/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 0.6470 - categorical_accuracy: 0.7983 - top_k_categorical_accuracy: 0.9739 - val_loss: 1.7670 - val_categorical_accuracy: 0.5665 - val_top_k_categorical_accuracy: 0.8320\n",
            "Epoch 49/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.6404 - categorical_accuracy: 0.7993 - top_k_categorical_accuracy: 0.9750 - val_loss: 1.8389 - val_categorical_accuracy: 0.5612 - val_top_k_categorical_accuracy: 0.8233\n",
            "Epoch 50/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.6350 - categorical_accuracy: 0.7998 - top_k_categorical_accuracy: 0.9747 - val_loss: 1.7622 - val_categorical_accuracy: 0.5727 - val_top_k_categorical_accuracy: 0.8304\n",
            "Epoch 51/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.6192 - categorical_accuracy: 0.8056 - top_k_categorical_accuracy: 0.9768 - val_loss: 1.7813 - val_categorical_accuracy: 0.5746 - val_top_k_categorical_accuracy: 0.8321\n",
            "Epoch 52/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.6083 - categorical_accuracy: 0.8088 - top_k_categorical_accuracy: 0.9768 - val_loss: 1.8025 - val_categorical_accuracy: 0.5731 - val_top_k_categorical_accuracy: 0.8294\n",
            "Epoch 53/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 0.5929 - categorical_accuracy: 0.8124 - top_k_categorical_accuracy: 0.9785 - val_loss: 1.7942 - val_categorical_accuracy: 0.5736 - val_top_k_categorical_accuracy: 0.8316\n",
            "Epoch 54/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 0.6001 - categorical_accuracy: 0.8124 - top_k_categorical_accuracy: 0.9780 - val_loss: 1.7724 - val_categorical_accuracy: 0.5704 - val_top_k_categorical_accuracy: 0.8318\n",
            "Epoch 55/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.5749 - categorical_accuracy: 0.8166 - top_k_categorical_accuracy: 0.9783 - val_loss: 1.8187 - val_categorical_accuracy: 0.5726 - val_top_k_categorical_accuracy: 0.8329\n",
            "Epoch 56/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 0.5569 - categorical_accuracy: 0.8234 - top_k_categorical_accuracy: 0.9817 - val_loss: 1.7900 - val_categorical_accuracy: 0.5717 - val_top_k_categorical_accuracy: 0.8309\n",
            "Epoch 57/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 0.5466 - categorical_accuracy: 0.8261 - top_k_categorical_accuracy: 0.9817 - val_loss: 1.8320 - val_categorical_accuracy: 0.5710 - val_top_k_categorical_accuracy: 0.8309\n",
            "Epoch 58/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 0.5395 - categorical_accuracy: 0.8281 - top_k_categorical_accuracy: 0.9823 - val_loss: 1.7939 - val_categorical_accuracy: 0.5746 - val_top_k_categorical_accuracy: 0.8299\n",
            "Epoch 59/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 0.5263 - categorical_accuracy: 0.8317 - top_k_categorical_accuracy: 0.9835 - val_loss: 1.8422 - val_categorical_accuracy: 0.5724 - val_top_k_categorical_accuracy: 0.8303\n",
            "Epoch 60/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 0.5176 - categorical_accuracy: 0.8355 - top_k_categorical_accuracy: 0.9840 - val_loss: 1.8441 - val_categorical_accuracy: 0.5734 - val_top_k_categorical_accuracy: 0.8347\n",
            "Epoch 61/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 0.5055 - categorical_accuracy: 0.8385 - top_k_categorical_accuracy: 0.9840 - val_loss: 1.8445 - val_categorical_accuracy: 0.5756 - val_top_k_categorical_accuracy: 0.8298\n",
            "Epoch 62/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.5025 - categorical_accuracy: 0.8404 - top_k_categorical_accuracy: 0.9842 - val_loss: 1.8828 - val_categorical_accuracy: 0.5757 - val_top_k_categorical_accuracy: 0.8308\n",
            "Epoch 63/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.5048 - categorical_accuracy: 0.8405 - top_k_categorical_accuracy: 0.9856 - val_loss: 1.7882 - val_categorical_accuracy: 0.5777 - val_top_k_categorical_accuracy: 0.8326\n",
            "Epoch 64/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.4772 - categorical_accuracy: 0.8472 - top_k_categorical_accuracy: 0.9866 - val_loss: 1.8631 - val_categorical_accuracy: 0.5753 - val_top_k_categorical_accuracy: 0.8306\n",
            "Epoch 65/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.4793 - categorical_accuracy: 0.8466 - top_k_categorical_accuracy: 0.9862 - val_loss: 1.8662 - val_categorical_accuracy: 0.5771 - val_top_k_categorical_accuracy: 0.8311\n",
            "Epoch 66/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.4665 - categorical_accuracy: 0.8491 - top_k_categorical_accuracy: 0.9869 - val_loss: 1.8362 - val_categorical_accuracy: 0.5809 - val_top_k_categorical_accuracy: 0.8370\n",
            "Epoch 67/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.4621 - categorical_accuracy: 0.8535 - top_k_categorical_accuracy: 0.9874 - val_loss: 1.8356 - val_categorical_accuracy: 0.5775 - val_top_k_categorical_accuracy: 0.8359\n",
            "Epoch 68/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.4619 - categorical_accuracy: 0.8533 - top_k_categorical_accuracy: 0.9871 - val_loss: 1.8447 - val_categorical_accuracy: 0.5755 - val_top_k_categorical_accuracy: 0.8330\n",
            "Epoch 69/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.4538 - categorical_accuracy: 0.8543 - top_k_categorical_accuracy: 0.9875 - val_loss: 1.8304 - val_categorical_accuracy: 0.5790 - val_top_k_categorical_accuracy: 0.8351\n",
            "Epoch 70/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.4438 - categorical_accuracy: 0.8586 - top_k_categorical_accuracy: 0.9876 - val_loss: 1.9060 - val_categorical_accuracy: 0.5801 - val_top_k_categorical_accuracy: 0.8318\n",
            "Epoch 71/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.4416 - categorical_accuracy: 0.8592 - top_k_categorical_accuracy: 0.9881 - val_loss: 1.8529 - val_categorical_accuracy: 0.5787 - val_top_k_categorical_accuracy: 0.8330\n",
            "Epoch 72/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 0.4386 - categorical_accuracy: 0.8590 - top_k_categorical_accuracy: 0.9878 - val_loss: 1.8431 - val_categorical_accuracy: 0.5798 - val_top_k_categorical_accuracy: 0.8334\n",
            "Epoch 73/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 0.4306 - categorical_accuracy: 0.8631 - top_k_categorical_accuracy: 0.9887 - val_loss: 1.8899 - val_categorical_accuracy: 0.5750 - val_top_k_categorical_accuracy: 0.8347\n",
            "Epoch 74/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 0.4200 - categorical_accuracy: 0.8661 - top_k_categorical_accuracy: 0.9895 - val_loss: 1.9257 - val_categorical_accuracy: 0.5769 - val_top_k_categorical_accuracy: 0.8367\n",
            "Epoch 75/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.4129 - categorical_accuracy: 0.8682 - top_k_categorical_accuracy: 0.9899 - val_loss: 1.8667 - val_categorical_accuracy: 0.5759 - val_top_k_categorical_accuracy: 0.8353\n",
            "Epoch 76/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.4211 - categorical_accuracy: 0.8661 - top_k_categorical_accuracy: 0.9897 - val_loss: 1.8753 - val_categorical_accuracy: 0.5769 - val_top_k_categorical_accuracy: 0.8371\n",
            "Epoch 77/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 0.4101 - categorical_accuracy: 0.8698 - top_k_categorical_accuracy: 0.9897 - val_loss: 1.9346 - val_categorical_accuracy: 0.5739 - val_top_k_categorical_accuracy: 0.8356\n",
            "Epoch 78/80\n",
            "77/77 [==============================] - 23s 292ms/step - loss: 0.3949 - categorical_accuracy: 0.8732 - top_k_categorical_accuracy: 0.9907 - val_loss: 1.8634 - val_categorical_accuracy: 0.5780 - val_top_k_categorical_accuracy: 0.8345\n",
            "Epoch 79/80\n",
            "77/77 [==============================] - 22s 292ms/step - loss: 0.3998 - categorical_accuracy: 0.8714 - top_k_categorical_accuracy: 0.9907 - val_loss: 1.9131 - val_categorical_accuracy: 0.5744 - val_top_k_categorical_accuracy: 0.8347\n",
            "Epoch 80/80\n",
            "77/77 [==============================] - 23s 293ms/step - loss: 0.3920 - categorical_accuracy: 0.8740 - top_k_categorical_accuracy: 0.9903 - val_loss: 1.9440 - val_categorical_accuracy: 0.5744 - val_top_k_categorical_accuracy: 0.8331\n",
            "\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.9440 - categorical_accuracy: 0.5744 - top_k_categorical_accuracy: 0.8331\n",
            "[1.9440486431121826, 0.574400007724762, 0.8331000208854675]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7TeJtPyt1ns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72948a5b-c66f-43ce-d254-eefffc55c821"
      },
      "source": [
        "#Modelo 3.1\n",
        "#Alterado de Dropout.\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU\n",
        "\n",
        "i = Input(shape=(x_train.shape[1],x_train.shape[2],3))\n",
        "\n",
        "#Añadimos la primera capa\n",
        "d = Conv2D(128,(3,3),activation='relu', padding='same')(i)\n",
        "d = Conv2D(128,(3,3),activation='relu', padding='same')(d)\n",
        "d = AveragePooling2D(pool_size=(2, 2))(d)\n",
        "d = Dropout(0.2)(d)\n",
        "\n",
        "#Añadimos la segunda capa\n",
        "d = Conv2D(256, (3,3), activation='relu', padding='same')(d)\n",
        "d = Conv2D(256, (3,3), activation='relu', padding='same')(d)\n",
        "d = AveragePooling2D(pool_size=(2, 2))(d)\n",
        "d = Dropout(0.25)(d)\n",
        "\n",
        "#Añadimos la tercera capa\n",
        "d = Conv2D(512,(3,3),activation='relu', padding='same')(d)\n",
        "d = Conv2D(512,(3,3),activation='relu', padding='same')(d)\n",
        "d = AveragePooling2D(pool_size=(2, 2))(d)\n",
        "d = Dropout(0.8)(d)\n",
        "\n",
        "#Aplanamos, y agregamos una capa densa, un dropout y la capa densa de salida.\n",
        "d = Flatten()(d)\n",
        "d = Dense(1024, activation='relu')(d)\n",
        "d = Dropout(0.5)(d)\n",
        "d = Dense(100, activation='softmax')(d)\n",
        "\n",
        "model = Model(inputs=i, outputs=d)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy','top_k_categorical_accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=80, batch_size=650, verbose=1, validation_data=(x_test,y_test))\n",
        "\n",
        "print()\n",
        "print(model.evaluate(x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 32, 32, 128)       3584      \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 32, 32, 128)       147584    \n",
            "_________________________________________________________________\n",
            "average_pooling2d_12 (Averag (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "average_pooling2d_13 (Averag (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 8, 8, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "average_pooling2d_14 (Averag (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1024)              8389632   \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 100)               102500    \n",
            "=================================================================\n",
            "Total params: 13,068,516\n",
            "Trainable params: 13,068,516\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/80\n",
            " 2/77 [..............................] - ETA: 18s - loss: 4.6415 - categorical_accuracy: 0.0185 - top_k_categorical_accuracy: 0.0692WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0934s vs `on_train_batch_end` time: 0.1921s). Check your callbacks.\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 4.4476 - categorical_accuracy: 0.0261 - top_k_categorical_accuracy: 0.1076 - val_loss: 4.1742 - val_categorical_accuracy: 0.0496 - val_top_k_categorical_accuracy: 0.1937\n",
            "Epoch 2/80\n",
            "77/77 [==============================] - 24s 305ms/step - loss: 4.0090 - categorical_accuracy: 0.0793 - top_k_categorical_accuracy: 0.2578 - val_loss: 3.7067 - val_categorical_accuracy: 0.1313 - val_top_k_categorical_accuracy: 0.3530\n",
            "Epoch 3/80\n",
            "77/77 [==============================] - 23s 301ms/step - loss: 3.6931 - categorical_accuracy: 0.1282 - top_k_categorical_accuracy: 0.3583 - val_loss: 3.4700 - val_categorical_accuracy: 0.1770 - val_top_k_categorical_accuracy: 0.4393\n",
            "Epoch 4/80\n",
            "77/77 [==============================] - 23s 302ms/step - loss: 3.4630 - categorical_accuracy: 0.1698 - top_k_categorical_accuracy: 0.4297 - val_loss: 3.1901 - val_categorical_accuracy: 0.2309 - val_top_k_categorical_accuracy: 0.5067\n",
            "Epoch 5/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 3.2409 - categorical_accuracy: 0.2096 - top_k_categorical_accuracy: 0.4892 - val_loss: 3.0004 - val_categorical_accuracy: 0.2720 - val_top_k_categorical_accuracy: 0.5674\n",
            "Epoch 6/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 3.0668 - categorical_accuracy: 0.2396 - top_k_categorical_accuracy: 0.5377 - val_loss: 2.8265 - val_categorical_accuracy: 0.2991 - val_top_k_categorical_accuracy: 0.5962\n",
            "Epoch 7/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 2.9307 - categorical_accuracy: 0.2676 - top_k_categorical_accuracy: 0.5713 - val_loss: 2.6853 - val_categorical_accuracy: 0.3259 - val_top_k_categorical_accuracy: 0.6290\n",
            "Epoch 8/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 2.7904 - categorical_accuracy: 0.2949 - top_k_categorical_accuracy: 0.6034 - val_loss: 2.5491 - val_categorical_accuracy: 0.3575 - val_top_k_categorical_accuracy: 0.6599\n",
            "Epoch 9/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 2.6757 - categorical_accuracy: 0.3171 - top_k_categorical_accuracy: 0.6312 - val_loss: 2.4650 - val_categorical_accuracy: 0.3708 - val_top_k_categorical_accuracy: 0.6725\n",
            "Epoch 10/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 2.5653 - categorical_accuracy: 0.3423 - top_k_categorical_accuracy: 0.6543 - val_loss: 2.3744 - val_categorical_accuracy: 0.3857 - val_top_k_categorical_accuracy: 0.6982\n",
            "Epoch 11/80\n",
            "77/77 [==============================] - 23s 303ms/step - loss: 2.4695 - categorical_accuracy: 0.3573 - top_k_categorical_accuracy: 0.6767 - val_loss: 2.3211 - val_categorical_accuracy: 0.4025 - val_top_k_categorical_accuracy: 0.7149\n",
            "Epoch 12/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 2.3736 - categorical_accuracy: 0.3773 - top_k_categorical_accuracy: 0.6969 - val_loss: 2.2477 - val_categorical_accuracy: 0.4197 - val_top_k_categorical_accuracy: 0.7251\n",
            "Epoch 13/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 2.3065 - categorical_accuracy: 0.3937 - top_k_categorical_accuracy: 0.7120 - val_loss: 2.1598 - val_categorical_accuracy: 0.4334 - val_top_k_categorical_accuracy: 0.7405\n",
            "Epoch 14/80\n",
            "77/77 [==============================] - 23s 303ms/step - loss: 2.2117 - categorical_accuracy: 0.4128 - top_k_categorical_accuracy: 0.7310 - val_loss: 2.0495 - val_categorical_accuracy: 0.4579 - val_top_k_categorical_accuracy: 0.7598\n",
            "Epoch 15/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 2.1346 - categorical_accuracy: 0.4262 - top_k_categorical_accuracy: 0.7490 - val_loss: 2.0542 - val_categorical_accuracy: 0.4648 - val_top_k_categorical_accuracy: 0.7670\n",
            "Epoch 16/80\n",
            "77/77 [==============================] - 23s 303ms/step - loss: 2.0779 - categorical_accuracy: 0.4435 - top_k_categorical_accuracy: 0.7599 - val_loss: 1.9703 - val_categorical_accuracy: 0.4707 - val_top_k_categorical_accuracy: 0.7766\n",
            "Epoch 17/80\n",
            "77/77 [==============================] - 23s 303ms/step - loss: 2.0078 - categorical_accuracy: 0.4571 - top_k_categorical_accuracy: 0.7718 - val_loss: 1.9356 - val_categorical_accuracy: 0.4831 - val_top_k_categorical_accuracy: 0.7816\n",
            "Epoch 18/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 1.9675 - categorical_accuracy: 0.4664 - top_k_categorical_accuracy: 0.7807 - val_loss: 1.9121 - val_categorical_accuracy: 0.4906 - val_top_k_categorical_accuracy: 0.7894\n",
            "Epoch 19/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 1.9116 - categorical_accuracy: 0.4738 - top_k_categorical_accuracy: 0.7903 - val_loss: 1.8962 - val_categorical_accuracy: 0.4914 - val_top_k_categorical_accuracy: 0.7878\n",
            "Epoch 20/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 1.8556 - categorical_accuracy: 0.4914 - top_k_categorical_accuracy: 0.7997 - val_loss: 1.8188 - val_categorical_accuracy: 0.5112 - val_top_k_categorical_accuracy: 0.8043\n",
            "Epoch 21/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 1.8033 - categorical_accuracy: 0.5023 - top_k_categorical_accuracy: 0.8109 - val_loss: 1.7792 - val_categorical_accuracy: 0.5213 - val_top_k_categorical_accuracy: 0.8138\n",
            "Epoch 22/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 1.7690 - categorical_accuracy: 0.5088 - top_k_categorical_accuracy: 0.8168 - val_loss: 1.7724 - val_categorical_accuracy: 0.5190 - val_top_k_categorical_accuracy: 0.8125\n",
            "Epoch 23/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 1.7276 - categorical_accuracy: 0.5192 - top_k_categorical_accuracy: 0.8238 - val_loss: 1.7617 - val_categorical_accuracy: 0.5199 - val_top_k_categorical_accuracy: 0.8125\n",
            "Epoch 24/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 1.6761 - categorical_accuracy: 0.5293 - top_k_categorical_accuracy: 0.8354 - val_loss: 1.7345 - val_categorical_accuracy: 0.5248 - val_top_k_categorical_accuracy: 0.8146\n",
            "Epoch 25/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 1.6411 - categorical_accuracy: 0.5390 - top_k_categorical_accuracy: 0.8406 - val_loss: 1.7053 - val_categorical_accuracy: 0.5321 - val_top_k_categorical_accuracy: 0.8250\n",
            "Epoch 26/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 1.6302 - categorical_accuracy: 0.5412 - top_k_categorical_accuracy: 0.8404 - val_loss: 1.6807 - val_categorical_accuracy: 0.5406 - val_top_k_categorical_accuracy: 0.8240\n",
            "Epoch 27/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 1.5810 - categorical_accuracy: 0.5531 - top_k_categorical_accuracy: 0.8500 - val_loss: 1.6672 - val_categorical_accuracy: 0.5421 - val_top_k_categorical_accuracy: 0.8284\n",
            "Epoch 28/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 1.5444 - categorical_accuracy: 0.5640 - top_k_categorical_accuracy: 0.8543 - val_loss: 1.6567 - val_categorical_accuracy: 0.5416 - val_top_k_categorical_accuracy: 0.8330\n",
            "Epoch 29/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 1.5248 - categorical_accuracy: 0.5648 - top_k_categorical_accuracy: 0.8592 - val_loss: 1.6401 - val_categorical_accuracy: 0.5445 - val_top_k_categorical_accuracy: 0.8344\n",
            "Epoch 30/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 1.4855 - categorical_accuracy: 0.5750 - top_k_categorical_accuracy: 0.8660 - val_loss: 1.6113 - val_categorical_accuracy: 0.5574 - val_top_k_categorical_accuracy: 0.8374\n",
            "Epoch 31/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 1.4553 - categorical_accuracy: 0.5813 - top_k_categorical_accuracy: 0.8703 - val_loss: 1.6331 - val_categorical_accuracy: 0.5533 - val_top_k_categorical_accuracy: 0.8359\n",
            "Epoch 32/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 1.4245 - categorical_accuracy: 0.5911 - top_k_categorical_accuracy: 0.8757 - val_loss: 1.6199 - val_categorical_accuracy: 0.5578 - val_top_k_categorical_accuracy: 0.8334\n",
            "Epoch 33/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 1.4033 - categorical_accuracy: 0.5940 - top_k_categorical_accuracy: 0.8781 - val_loss: 1.5915 - val_categorical_accuracy: 0.5623 - val_top_k_categorical_accuracy: 0.8409\n",
            "Epoch 34/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 1.3679 - categorical_accuracy: 0.6030 - top_k_categorical_accuracy: 0.8831 - val_loss: 1.5866 - val_categorical_accuracy: 0.5645 - val_top_k_categorical_accuracy: 0.8427\n",
            "Epoch 35/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 1.3452 - categorical_accuracy: 0.6069 - top_k_categorical_accuracy: 0.8874 - val_loss: 1.5873 - val_categorical_accuracy: 0.5633 - val_top_k_categorical_accuracy: 0.8411\n",
            "Epoch 36/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 1.3163 - categorical_accuracy: 0.6148 - top_k_categorical_accuracy: 0.8913 - val_loss: 1.5735 - val_categorical_accuracy: 0.5683 - val_top_k_categorical_accuracy: 0.8455\n",
            "Epoch 37/80\n",
            "77/77 [==============================] - 24s 305ms/step - loss: 1.3060 - categorical_accuracy: 0.6179 - top_k_categorical_accuracy: 0.8946 - val_loss: 1.5673 - val_categorical_accuracy: 0.5687 - val_top_k_categorical_accuracy: 0.8436\n",
            "Epoch 38/80\n",
            "77/77 [==============================] - 24s 307ms/step - loss: 1.2753 - categorical_accuracy: 0.6244 - top_k_categorical_accuracy: 0.8975 - val_loss: 1.5613 - val_categorical_accuracy: 0.5690 - val_top_k_categorical_accuracy: 0.8456\n",
            "Epoch 39/80\n",
            "77/77 [==============================] - 23s 305ms/step - loss: 1.2502 - categorical_accuracy: 0.6343 - top_k_categorical_accuracy: 0.9022 - val_loss: 1.5558 - val_categorical_accuracy: 0.5700 - val_top_k_categorical_accuracy: 0.8425\n",
            "Epoch 40/80\n",
            "77/77 [==============================] - 23s 305ms/step - loss: 1.2387 - categorical_accuracy: 0.6333 - top_k_categorical_accuracy: 0.9025 - val_loss: 1.5499 - val_categorical_accuracy: 0.5714 - val_top_k_categorical_accuracy: 0.8443\n",
            "Epoch 41/80\n",
            "77/77 [==============================] - 24s 306ms/step - loss: 1.2192 - categorical_accuracy: 0.6385 - top_k_categorical_accuracy: 0.9051 - val_loss: 1.5412 - val_categorical_accuracy: 0.5762 - val_top_k_categorical_accuracy: 0.8475\n",
            "Epoch 42/80\n",
            "77/77 [==============================] - 24s 306ms/step - loss: 1.1907 - categorical_accuracy: 0.6461 - top_k_categorical_accuracy: 0.9096 - val_loss: 1.5390 - val_categorical_accuracy: 0.5779 - val_top_k_categorical_accuracy: 0.8489\n",
            "Epoch 43/80\n",
            "77/77 [==============================] - 24s 305ms/step - loss: 1.1654 - categorical_accuracy: 0.6541 - top_k_categorical_accuracy: 0.9121 - val_loss: 1.5256 - val_categorical_accuracy: 0.5781 - val_top_k_categorical_accuracy: 0.8499\n",
            "Epoch 44/80\n",
            "77/77 [==============================] - 23s 305ms/step - loss: 1.1595 - categorical_accuracy: 0.6549 - top_k_categorical_accuracy: 0.9143 - val_loss: 1.5579 - val_categorical_accuracy: 0.5787 - val_top_k_categorical_accuracy: 0.8484\n",
            "Epoch 45/80\n",
            "77/77 [==============================] - 23s 305ms/step - loss: 1.1428 - categorical_accuracy: 0.6592 - top_k_categorical_accuracy: 0.9169 - val_loss: 1.5558 - val_categorical_accuracy: 0.5810 - val_top_k_categorical_accuracy: 0.8467\n",
            "Epoch 46/80\n",
            "77/77 [==============================] - 23s 305ms/step - loss: 1.1291 - categorical_accuracy: 0.6614 - top_k_categorical_accuracy: 0.9179 - val_loss: 1.5148 - val_categorical_accuracy: 0.5822 - val_top_k_categorical_accuracy: 0.8506\n",
            "Epoch 47/80\n",
            "77/77 [==============================] - 23s 305ms/step - loss: 1.0966 - categorical_accuracy: 0.6690 - top_k_categorical_accuracy: 0.9230 - val_loss: 1.5042 - val_categorical_accuracy: 0.5865 - val_top_k_categorical_accuracy: 0.8564\n",
            "Epoch 48/80\n",
            "77/77 [==============================] - 24s 305ms/step - loss: 1.0917 - categorical_accuracy: 0.6719 - top_k_categorical_accuracy: 0.9240 - val_loss: 1.5210 - val_categorical_accuracy: 0.5827 - val_top_k_categorical_accuracy: 0.8533\n",
            "Epoch 49/80\n",
            "77/77 [==============================] - 24s 306ms/step - loss: 1.0757 - categorical_accuracy: 0.6756 - top_k_categorical_accuracy: 0.9254 - val_loss: 1.5185 - val_categorical_accuracy: 0.5881 - val_top_k_categorical_accuracy: 0.8534\n",
            "Epoch 50/80\n",
            "77/77 [==============================] - 24s 306ms/step - loss: 1.0557 - categorical_accuracy: 0.6811 - top_k_categorical_accuracy: 0.9285 - val_loss: 1.5236 - val_categorical_accuracy: 0.5837 - val_top_k_categorical_accuracy: 0.8543\n",
            "Epoch 51/80\n",
            "77/77 [==============================] - 24s 306ms/step - loss: 1.0460 - categorical_accuracy: 0.6834 - top_k_categorical_accuracy: 0.9304 - val_loss: 1.5114 - val_categorical_accuracy: 0.5929 - val_top_k_categorical_accuracy: 0.8554\n",
            "Epoch 52/80\n",
            "77/77 [==============================] - 24s 306ms/step - loss: 1.0387 - categorical_accuracy: 0.6855 - top_k_categorical_accuracy: 0.9326 - val_loss: 1.5254 - val_categorical_accuracy: 0.5857 - val_top_k_categorical_accuracy: 0.8539\n",
            "Epoch 53/80\n",
            "77/77 [==============================] - 24s 305ms/step - loss: 1.0073 - categorical_accuracy: 0.6937 - top_k_categorical_accuracy: 0.9347 - val_loss: 1.5277 - val_categorical_accuracy: 0.5889 - val_top_k_categorical_accuracy: 0.8512\n",
            "Epoch 54/80\n",
            "77/77 [==============================] - 24s 306ms/step - loss: 0.9923 - categorical_accuracy: 0.6969 - top_k_categorical_accuracy: 0.9373 - val_loss: 1.5294 - val_categorical_accuracy: 0.5850 - val_top_k_categorical_accuracy: 0.8517\n",
            "Epoch 55/80\n",
            "77/77 [==============================] - 24s 306ms/step - loss: 0.9916 - categorical_accuracy: 0.6988 - top_k_categorical_accuracy: 0.9374 - val_loss: 1.5230 - val_categorical_accuracy: 0.5850 - val_top_k_categorical_accuracy: 0.8486\n",
            "Epoch 56/80\n",
            "77/77 [==============================] - 24s 307ms/step - loss: 0.9678 - categorical_accuracy: 0.7042 - top_k_categorical_accuracy: 0.9391 - val_loss: 1.5117 - val_categorical_accuracy: 0.5952 - val_top_k_categorical_accuracy: 0.8556\n",
            "Epoch 57/80\n",
            "77/77 [==============================] - 24s 306ms/step - loss: 0.9585 - categorical_accuracy: 0.7054 - top_k_categorical_accuracy: 0.9420 - val_loss: 1.4991 - val_categorical_accuracy: 0.5948 - val_top_k_categorical_accuracy: 0.8541\n",
            "Epoch 58/80\n",
            "77/77 [==============================] - 24s 306ms/step - loss: 0.9607 - categorical_accuracy: 0.7082 - top_k_categorical_accuracy: 0.9410 - val_loss: 1.5047 - val_categorical_accuracy: 0.5927 - val_top_k_categorical_accuracy: 0.8538\n",
            "Epoch 59/80\n",
            "77/77 [==============================] - 24s 306ms/step - loss: 0.9395 - categorical_accuracy: 0.7131 - top_k_categorical_accuracy: 0.9441 - val_loss: 1.5303 - val_categorical_accuracy: 0.5893 - val_top_k_categorical_accuracy: 0.8479\n",
            "Epoch 60/80\n",
            "77/77 [==============================] - 24s 306ms/step - loss: 0.9279 - categorical_accuracy: 0.7136 - top_k_categorical_accuracy: 0.9451 - val_loss: 1.5155 - val_categorical_accuracy: 0.5940 - val_top_k_categorical_accuracy: 0.8541\n",
            "Epoch 61/80\n",
            "77/77 [==============================] - 24s 307ms/step - loss: 0.9241 - categorical_accuracy: 0.7186 - top_k_categorical_accuracy: 0.9457 - val_loss: 1.5223 - val_categorical_accuracy: 0.5919 - val_top_k_categorical_accuracy: 0.8513\n",
            "Epoch 62/80\n",
            "77/77 [==============================] - 24s 306ms/step - loss: 0.9002 - categorical_accuracy: 0.7240 - top_k_categorical_accuracy: 0.9476 - val_loss: 1.5435 - val_categorical_accuracy: 0.5896 - val_top_k_categorical_accuracy: 0.8554\n",
            "Epoch 63/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 0.8952 - categorical_accuracy: 0.7244 - top_k_categorical_accuracy: 0.9494 - val_loss: 1.5485 - val_categorical_accuracy: 0.5957 - val_top_k_categorical_accuracy: 0.8524\n",
            "Epoch 64/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 0.8837 - categorical_accuracy: 0.7246 - top_k_categorical_accuracy: 0.9505 - val_loss: 1.5074 - val_categorical_accuracy: 0.5970 - val_top_k_categorical_accuracy: 0.8563\n",
            "Epoch 65/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 0.8710 - categorical_accuracy: 0.7297 - top_k_categorical_accuracy: 0.9521 - val_loss: 1.5228 - val_categorical_accuracy: 0.5973 - val_top_k_categorical_accuracy: 0.8574\n",
            "Epoch 66/80\n",
            "77/77 [==============================] - 23s 305ms/step - loss: 0.8636 - categorical_accuracy: 0.7331 - top_k_categorical_accuracy: 0.9525 - val_loss: 1.5039 - val_categorical_accuracy: 0.5991 - val_top_k_categorical_accuracy: 0.8569\n",
            "Epoch 67/80\n",
            "77/77 [==============================] - 23s 305ms/step - loss: 0.8416 - categorical_accuracy: 0.7384 - top_k_categorical_accuracy: 0.9556 - val_loss: 1.5285 - val_categorical_accuracy: 0.5995 - val_top_k_categorical_accuracy: 0.8554\n",
            "Epoch 68/80\n",
            "77/77 [==============================] - 24s 305ms/step - loss: 0.8360 - categorical_accuracy: 0.7409 - top_k_categorical_accuracy: 0.9549 - val_loss: 1.5118 - val_categorical_accuracy: 0.5964 - val_top_k_categorical_accuracy: 0.8577\n",
            "Epoch 69/80\n",
            "77/77 [==============================] - 23s 305ms/step - loss: 0.8327 - categorical_accuracy: 0.7410 - top_k_categorical_accuracy: 0.9573 - val_loss: 1.5434 - val_categorical_accuracy: 0.5970 - val_top_k_categorical_accuracy: 0.8527\n",
            "Epoch 70/80\n",
            "77/77 [==============================] - 24s 306ms/step - loss: 0.8265 - categorical_accuracy: 0.7421 - top_k_categorical_accuracy: 0.9564 - val_loss: 1.5035 - val_categorical_accuracy: 0.6017 - val_top_k_categorical_accuracy: 0.8561\n",
            "Epoch 71/80\n",
            "77/77 [==============================] - 23s 305ms/step - loss: 0.8113 - categorical_accuracy: 0.7475 - top_k_categorical_accuracy: 0.9582 - val_loss: 1.5411 - val_categorical_accuracy: 0.5964 - val_top_k_categorical_accuracy: 0.8544\n",
            "Epoch 72/80\n",
            "77/77 [==============================] - 23s 305ms/step - loss: 0.8066 - categorical_accuracy: 0.7482 - top_k_categorical_accuracy: 0.9592 - val_loss: 1.5455 - val_categorical_accuracy: 0.6025 - val_top_k_categorical_accuracy: 0.8555\n",
            "Epoch 73/80\n",
            "77/77 [==============================] - 23s 305ms/step - loss: 0.7970 - categorical_accuracy: 0.7518 - top_k_categorical_accuracy: 0.9596 - val_loss: 1.5108 - val_categorical_accuracy: 0.6022 - val_top_k_categorical_accuracy: 0.8598\n",
            "Epoch 74/80\n",
            "77/77 [==============================] - 23s 305ms/step - loss: 0.7825 - categorical_accuracy: 0.7542 - top_k_categorical_accuracy: 0.9606 - val_loss: 1.5315 - val_categorical_accuracy: 0.6037 - val_top_k_categorical_accuracy: 0.8524\n",
            "Epoch 75/80\n",
            "77/77 [==============================] - 23s 305ms/step - loss: 0.7836 - categorical_accuracy: 0.7529 - top_k_categorical_accuracy: 0.9613 - val_loss: 1.5196 - val_categorical_accuracy: 0.5986 - val_top_k_categorical_accuracy: 0.8573\n",
            "Epoch 76/80\n",
            "77/77 [==============================] - 23s 305ms/step - loss: 0.7806 - categorical_accuracy: 0.7556 - top_k_categorical_accuracy: 0.9614 - val_loss: 1.5418 - val_categorical_accuracy: 0.5960 - val_top_k_categorical_accuracy: 0.8537\n",
            "Epoch 77/80\n",
            "77/77 [==============================] - 23s 305ms/step - loss: 0.7687 - categorical_accuracy: 0.7592 - top_k_categorical_accuracy: 0.9620 - val_loss: 1.5240 - val_categorical_accuracy: 0.6016 - val_top_k_categorical_accuracy: 0.8565\n",
            "Epoch 78/80\n",
            "77/77 [==============================] - 23s 305ms/step - loss: 0.7465 - categorical_accuracy: 0.7670 - top_k_categorical_accuracy: 0.9650 - val_loss: 1.5470 - val_categorical_accuracy: 0.6018 - val_top_k_categorical_accuracy: 0.8559\n",
            "Epoch 79/80\n",
            "77/77 [==============================] - 23s 304ms/step - loss: 0.7500 - categorical_accuracy: 0.7640 - top_k_categorical_accuracy: 0.9644 - val_loss: 1.5485 - val_categorical_accuracy: 0.5991 - val_top_k_categorical_accuracy: 0.8534\n",
            "Epoch 80/80\n",
            "77/77 [==============================] - 23s 305ms/step - loss: 0.7500 - categorical_accuracy: 0.7663 - top_k_categorical_accuracy: 0.9644 - val_loss: 1.5058 - val_categorical_accuracy: 0.6084 - val_top_k_categorical_accuracy: 0.8575\n",
            "\n",
            "313/313 [==============================] - 2s 8ms/step - loss: 1.5058 - categorical_accuracy: 0.6084 - top_k_categorical_accuracy: 0.8575\n",
            "[1.5057657957077026, 0.6083999872207642, 0.8575000166893005]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Lf8IoEUQR3-"
      },
      "source": [
        "Con lo mostrado anteriormente arme la red neuronal de arriba pero es demasiado lenta pero en el **modelo tres** con 80 epochs llegue a conseguir una precisión de 87.40% para el set de entrenamiento y del 57.44% para el set de validación.\n",
        "Agrege un modelo 3.1 cambiando solo el dropout de la tercera capa de 0.5 a 0.8  llegamos a conseguir una precisión de 76.63% que cayo con respecto al anterior para el set de entrenamiento y del 60.84% para el set de validación. \n",
        "Un poco mejor que las anteriores, pero es mucho mas lenta. \n",
        "\n",
        "Por ende me quedaria con el modelo 3, ya que el modelo 3.1 perdi 10.77% en el set de entrenamiento y gane nada mas que 3.4% en el set de validacion. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cVnwH0ocszZ"
      },
      "source": [
        "La anterior a ser muy lenta, trate de armar otra que sea relativamente parecida pero un poco mas rapida.\n",
        "La presentada a continuacion arrancamos con:\n",
        "* En la capa uno con una **Conv2D** de 32 filtros con una funcion de activacion 'Relu' como ya veniamos usando, le agrege la función **Leaky ReLU** transforma los valores introducidos multiplicando los negativos por un coeficiente rectificativo y dejando los positivos según entran, tambien agrege una normalizacion **BatchNormalization** que básicamente lo que trate de hacer es añadir un paso extra, habitualmente entre las neuronas y la función de activación, con la idea de normalizar las activaciones de salida, y al final un **AveragePooling** como ya veniamos usando.\n",
        "* En la capa numero dos, use lo mismo alterando solo la cantidad de filtros a 64 en la Conv2D, y ademas agregar un pool_size en el averagePooling de (2,2).\n",
        "* En la capa numero tres, use lo mismo tambien alterando la cantidad de filtros a 128 en la conv2D y ademas agregando un pool_size en el averagePooling de (3,3)\n",
        "* En la capa numero cuatro, use lo mismo tambien pero alterando la cantidad de filtros a 192 en la conv2D y en esta capa saque la normalizacion ya que al sacarla aumentaba considerablemente los valores en la etapa de entrenamiento, y por ende en la etapa de verificacion, por ultimo agregamos un Droup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6IzESWkqyp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f4caf5d-d6d9-48f1-edbd-166352f0cafe"
      },
      "source": [
        "#MODELO 4\n",
        "\n",
        "i = Input(shape=(x_train.shape[1],x_train.shape[2],3))\n",
        "\n",
        "#Capa 1\n",
        "d = Conv2D(32,(3,3),activation='relu', strides=(1,1),  name='conv_1')(i)\n",
        "d = LeakyReLU(alpha=0.3)(d)\n",
        "d = BatchNormalization(name='norm_1')(d)\n",
        "d = AveragePooling2D(pool_size=(1,1))(d)\n",
        "\n",
        "#Capa 2\n",
        "d = Conv2D(64,(3,3),activation='relu', name='conv_2')(d)\n",
        "d = LeakyReLU(alpha=0.3)(d)\n",
        "d = BatchNormalization(name='norm_2')(d)\n",
        "d = AveragePooling2D(pool_size=(2,2))(d)\n",
        "\n",
        "#Capa 3\n",
        "d = Conv2D(128,(3,3),activation='relu', name='conv_3')(d)\n",
        "d = LeakyReLU(alpha=0.3)(d)\n",
        "d = BatchNormalization(name='norm_3')(d)\n",
        "d = AveragePooling2D(pool_size=(3, 3))(d)\n",
        "\n",
        "#Capa 4\n",
        "d = Conv2D(192,(3,3),activation='relu',name='conv_4')(d)\n",
        "d = LeakyReLU(alpha=0.3)(d)\n",
        "d = AveragePooling2D(pool_size=(2, 2))(d)\n",
        "\n",
        "#Aplanar\n",
        "d = Flatten()(d)\n",
        "d = Dropout(0.1)(d)\n",
        "#Capa de salida\n",
        "d = Dense(100,activation='softmax')(d)\n",
        "\n",
        "model = Model(inputs=i, outputs=d)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy','top_k_categorical_accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=35, batch_size=645, verbose=1, validation_data=(x_test,y_test))\n",
        "\n",
        "print()\n",
        "print(model.evaluate(x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "conv_1 (Conv2D)              (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "norm_1 (BatchNormalization)  (None, 30, 30, 32)        128       \n",
            "_________________________________________________________________\n",
            "average_pooling2d_23 (Averag (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv_2 (Conv2D)              (None, 28, 28, 64)        18496     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "norm_2 (BatchNormalization)  (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "average_pooling2d_24 (Averag (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv_3 (Conv2D)              (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "norm_3 (BatchNormalization)  (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "average_pooling2d_25 (Averag (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv_4 (Conv2D)              (None, 2, 2, 192)         221376    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 2, 2, 192)         0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_26 (Averag (None, 1, 1, 192)         0         \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 192)               0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 192)               0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 100)               19300     \n",
            "=================================================================\n",
            "Total params: 334,820\n",
            "Trainable params: 334,372\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "Epoch 1/35\n",
            " 2/78 [..............................] - ETA: 5s - loss: 4.6751 - categorical_accuracy: 0.0217 - top_k_categorical_accuracy: 0.0760WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0309s vs `on_train_batch_end` time: 0.0479s). Check your callbacks.\n",
            "78/78 [==============================] - 6s 76ms/step - loss: 3.6401 - categorical_accuracy: 0.1594 - top_k_categorical_accuracy: 0.3883 - val_loss: 4.8254 - val_categorical_accuracy: 0.0100 - val_top_k_categorical_accuracy: 0.0541\n",
            "Epoch 2/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 2.8545 - categorical_accuracy: 0.2921 - top_k_categorical_accuracy: 0.5940 - val_loss: 4.9576 - val_categorical_accuracy: 0.0119 - val_top_k_categorical_accuracy: 0.0599\n",
            "Epoch 3/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 2.4664 - categorical_accuracy: 0.3676 - top_k_categorical_accuracy: 0.6798 - val_loss: 5.3178 - val_categorical_accuracy: 0.0171 - val_top_k_categorical_accuracy: 0.0628\n",
            "Epoch 4/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 2.2113 - categorical_accuracy: 0.4247 - top_k_categorical_accuracy: 0.7335 - val_loss: 5.5396 - val_categorical_accuracy: 0.0114 - val_top_k_categorical_accuracy: 0.0701\n",
            "Epoch 5/35\n",
            "78/78 [==============================] - 6s 74ms/step - loss: 2.0221 - categorical_accuracy: 0.4671 - top_k_categorical_accuracy: 0.7710 - val_loss: 5.8894 - val_categorical_accuracy: 0.0206 - val_top_k_categorical_accuracy: 0.1161\n",
            "Epoch 6/35\n",
            "78/78 [==============================] - 6s 74ms/step - loss: 1.8662 - categorical_accuracy: 0.5007 - top_k_categorical_accuracy: 0.7990 - val_loss: 5.2750 - val_categorical_accuracy: 0.0443 - val_top_k_categorical_accuracy: 0.1857\n",
            "Epoch 7/35\n",
            "78/78 [==============================] - 6s 74ms/step - loss: 1.7403 - categorical_accuracy: 0.5332 - top_k_categorical_accuracy: 0.8213 - val_loss: 3.6765 - val_categorical_accuracy: 0.1831 - val_top_k_categorical_accuracy: 0.4547\n",
            "Epoch 8/35\n",
            "78/78 [==============================] - 6s 74ms/step - loss: 1.6333 - categorical_accuracy: 0.5569 - top_k_categorical_accuracy: 0.8378 - val_loss: 2.5796 - val_categorical_accuracy: 0.3511 - val_top_k_categorical_accuracy: 0.6523\n",
            "Epoch 9/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 1.5384 - categorical_accuracy: 0.5775 - top_k_categorical_accuracy: 0.8540 - val_loss: 2.0136 - val_categorical_accuracy: 0.4729 - val_top_k_categorical_accuracy: 0.7698\n",
            "Epoch 10/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 1.4432 - categorical_accuracy: 0.6018 - top_k_categorical_accuracy: 0.8682 - val_loss: 1.9465 - val_categorical_accuracy: 0.4849 - val_top_k_categorical_accuracy: 0.7859\n",
            "Epoch 11/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 1.3629 - categorical_accuracy: 0.6214 - top_k_categorical_accuracy: 0.8807 - val_loss: 1.8651 - val_categorical_accuracy: 0.5088 - val_top_k_categorical_accuracy: 0.7979\n",
            "Epoch 12/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 1.2835 - categorical_accuracy: 0.6434 - top_k_categorical_accuracy: 0.8905 - val_loss: 1.8069 - val_categorical_accuracy: 0.5263 - val_top_k_categorical_accuracy: 0.8080\n",
            "Epoch 13/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 1.2122 - categorical_accuracy: 0.6595 - top_k_categorical_accuracy: 0.9032 - val_loss: 1.7852 - val_categorical_accuracy: 0.5303 - val_top_k_categorical_accuracy: 0.8093\n",
            "Epoch 14/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 1.1507 - categorical_accuracy: 0.6738 - top_k_categorical_accuracy: 0.9124 - val_loss: 1.8718 - val_categorical_accuracy: 0.5114 - val_top_k_categorical_accuracy: 0.7971\n",
            "Epoch 15/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 1.0945 - categorical_accuracy: 0.6901 - top_k_categorical_accuracy: 0.9189 - val_loss: 1.7783 - val_categorical_accuracy: 0.5334 - val_top_k_categorical_accuracy: 0.8117\n",
            "Epoch 16/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 1.0344 - categorical_accuracy: 0.7063 - top_k_categorical_accuracy: 0.9277 - val_loss: 1.8045 - val_categorical_accuracy: 0.5282 - val_top_k_categorical_accuracy: 0.8119\n",
            "Epoch 17/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 0.9807 - categorical_accuracy: 0.7176 - top_k_categorical_accuracy: 0.9338 - val_loss: 1.8564 - val_categorical_accuracy: 0.5269 - val_top_k_categorical_accuracy: 0.8082\n",
            "Epoch 18/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 0.9352 - categorical_accuracy: 0.7292 - top_k_categorical_accuracy: 0.9400 - val_loss: 1.9128 - val_categorical_accuracy: 0.5170 - val_top_k_categorical_accuracy: 0.7999\n",
            "Epoch 19/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 0.8902 - categorical_accuracy: 0.7428 - top_k_categorical_accuracy: 0.9442 - val_loss: 1.8702 - val_categorical_accuracy: 0.5253 - val_top_k_categorical_accuracy: 0.8086\n",
            "Epoch 20/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 0.8339 - categorical_accuracy: 0.7569 - top_k_categorical_accuracy: 0.9518 - val_loss: 1.8346 - val_categorical_accuracy: 0.5400 - val_top_k_categorical_accuracy: 0.8149\n",
            "Epoch 21/35\n",
            "78/78 [==============================] - 6s 74ms/step - loss: 0.7870 - categorical_accuracy: 0.7683 - top_k_categorical_accuracy: 0.9577 - val_loss: 1.9241 - val_categorical_accuracy: 0.5287 - val_top_k_categorical_accuracy: 0.8076\n",
            "Epoch 22/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 0.7454 - categorical_accuracy: 0.7835 - top_k_categorical_accuracy: 0.9610 - val_loss: 1.8563 - val_categorical_accuracy: 0.5470 - val_top_k_categorical_accuracy: 0.8174\n",
            "Epoch 23/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 0.7009 - categorical_accuracy: 0.7947 - top_k_categorical_accuracy: 0.9651 - val_loss: 1.8567 - val_categorical_accuracy: 0.5441 - val_top_k_categorical_accuracy: 0.8154\n",
            "Epoch 24/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 0.6642 - categorical_accuracy: 0.8039 - top_k_categorical_accuracy: 0.9692 - val_loss: 1.8638 - val_categorical_accuracy: 0.5428 - val_top_k_categorical_accuracy: 0.8220\n",
            "Epoch 25/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 0.6340 - categorical_accuracy: 0.8133 - top_k_categorical_accuracy: 0.9720 - val_loss: 1.9125 - val_categorical_accuracy: 0.5444 - val_top_k_categorical_accuracy: 0.8140\n",
            "Epoch 26/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 0.5947 - categorical_accuracy: 0.8238 - top_k_categorical_accuracy: 0.9763 - val_loss: 2.0216 - val_categorical_accuracy: 0.5319 - val_top_k_categorical_accuracy: 0.8052\n",
            "Epoch 27/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 0.5678 - categorical_accuracy: 0.8292 - top_k_categorical_accuracy: 0.9791 - val_loss: 1.9953 - val_categorical_accuracy: 0.5359 - val_top_k_categorical_accuracy: 0.8068\n",
            "Epoch 28/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 0.5342 - categorical_accuracy: 0.8397 - top_k_categorical_accuracy: 0.9813 - val_loss: 1.9944 - val_categorical_accuracy: 0.5380 - val_top_k_categorical_accuracy: 0.8090\n",
            "Epoch 29/35\n",
            "78/78 [==============================] - 6s 72ms/step - loss: 0.5052 - categorical_accuracy: 0.8481 - top_k_categorical_accuracy: 0.9839 - val_loss: 2.0113 - val_categorical_accuracy: 0.5409 - val_top_k_categorical_accuracy: 0.8150\n",
            "Epoch 30/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 0.4723 - categorical_accuracy: 0.8594 - top_k_categorical_accuracy: 0.9857 - val_loss: 2.0201 - val_categorical_accuracy: 0.5418 - val_top_k_categorical_accuracy: 0.8139\n",
            "Epoch 31/35\n",
            "78/78 [==============================] - 6s 72ms/step - loss: 0.4413 - categorical_accuracy: 0.8673 - top_k_categorical_accuracy: 0.9883 - val_loss: 2.0489 - val_categorical_accuracy: 0.5434 - val_top_k_categorical_accuracy: 0.8104\n",
            "Epoch 32/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 0.4252 - categorical_accuracy: 0.8731 - top_k_categorical_accuracy: 0.9888 - val_loss: 2.0994 - val_categorical_accuracy: 0.5355 - val_top_k_categorical_accuracy: 0.8087\n",
            "Epoch 33/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 0.4013 - categorical_accuracy: 0.8798 - top_k_categorical_accuracy: 0.9905 - val_loss: 2.2074 - val_categorical_accuracy: 0.5280 - val_top_k_categorical_accuracy: 0.7971\n",
            "Epoch 34/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 0.3868 - categorical_accuracy: 0.8848 - top_k_categorical_accuracy: 0.9914 - val_loss: 2.1718 - val_categorical_accuracy: 0.5348 - val_top_k_categorical_accuracy: 0.8028\n",
            "Epoch 35/35\n",
            "78/78 [==============================] - 6s 73ms/step - loss: 0.3574 - categorical_accuracy: 0.8922 - top_k_categorical_accuracy: 0.9940 - val_loss: 2.2480 - val_categorical_accuracy: 0.5288 - val_top_k_categorical_accuracy: 0.8044\n",
            "\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.2480 - categorical_accuracy: 0.5288 - top_k_categorical_accuracy: 0.8044\n",
            "[2.2480194568634033, 0.5288000106811523, 0.8044000267982483]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKkbiRY0uI4r"
      },
      "source": [
        "Considerablemente mas rapida que la anterior, pasamos a 6s en cada epoch y en la anterior estaba en 23s cada epoch, y llegamos a conseguir una precisión de 89.22% para el set de entrenamiento y del 52.88% para el set de validación. Ganamos en precision para el set de entrenamiendo pero perdimos un poco en el porcentaje en la validacion, pero ganamos mucho mas tiempo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24023k8nunNY"
      },
      "source": [
        "# **Conclusion**\n",
        "\n",
        "Como conclusion final vimos en lo anterior explicado las distintas alternativas que tenemos, como resumen tenemos:\n",
        "* **Modelo 1:** El modelo alcanza una precisión de 67.77% para el set de entrenamiento y del 35.68% para el set de validación.\n",
        "* **Modelo 2:** El modelo alcanza una precisión de 45.23% para el set de entrenamiento y del 37.10% para el set de validación.\n",
        "* **Modelo 3:** El modelo alcanza una precisión de 87.40% para el set de entrenamiento y del 57.44% para el set de validación.\n",
        "  * **Modelo 3.1:** El modelo alcanza una precisión de 76.63% para el set de entrenamiento y del 60.84% para el set de validación.\n",
        "* **Modelo 4:** El modelo alcanza una precisión de 89.22% para el set de entrenamiento y del 52.88% para el set de validación.\n",
        "\n",
        "Despues de ver y analizar estos cinco modelos, creo que la opcion mas logica seria quedarnos con el modelo 3, ya que nos da una precision del 87.40% para el set de entrenamiento y ademas nos da un 57.44% para el set de validacion, lo malo de esta red es el tiempo que lleva llegar a estos porcentajes, por ejemplo, el modelo 4 lo arme pensando en achichar ese tiempo y obtuve un porcentaje mayor para el set de entrenamiento pero menor en validacion, entonces como para cerrar seria mejor elegir el modelo 3 o modelo 4.\n",
        "\n",
        "Como para cerrar este trabajo me gustaria decir que pude entender como funciona una red neuronal y poder realizarla, modificar y encarar un problema solo me parecio un reto bastante grande en mi desarrollo personal, definitivamente no llegue a un valor mas alto que el mostrado pero yo creo que con un poco mas de tiempo hubiera llegado un poco mas, ya que a estas fechas entre los demas parciales se complica, pero estoy contento con el resultado final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsowmYvuHsrl"
      },
      "source": [
        "# Bibliografia usada:\n",
        "\n",
        "* https://stackoverflow.com/questions/59749499/how-to-disable-the-warning-tensorflowmethod-on-train-batch-end-is-slow-compa\n",
        "* https://datasmarts.net/es/como-entrenar-una-red-neuronal-convolucional-cnn-en-cifar-10-con-keras/\n",
        "* https://github.com/codificandobits/Clasificacion_MNIST_RedesConvolucionales_Keras/blob/master/Clasificacion_de_Digitos_Usando_Redes_Convolucionales_y_Keras.ipynb\n",
        "* http://oa.upm.es/53050/1/TFG_JAVIER_MARTINEZ_LLAMAS.pdf\n",
        "* https://github.com/pjreddie/darknet/blob/master/python/darknet.py\n",
        "* http://opac.pucv.cl/pucv_txt/txt-4000/UCC4151_01.pdf\n",
        "* https://andrewkruger.github.io/projects/2017-08-05-keras-convolutional-neural-network-for-cifar-100#the-model\n",
        "* https://www.aprendemachinelearning.com/deteccion-de-objetos-con-python-yolo-keras-tutorial/\n",
        "* https://towardsdatascience.com/image-recognition-with-machine-learning-on-python-convolutional-neural-network-363073020588\n",
        "* https://github.com/andrewkruger/cifar100_CNN\n",
        "* https://hirogosomewhere.com/2020/05/16/cifar100-image-classification/\n",
        "* https://medium.com/metadatos/t%C3%A9cnicas-de-regularizaci%C3%B3n-b%C3%A1sicas-para-redes-neuronales-b48f396924d4#:~:text=Normalizaci%C3%B3n%20por%20lotes%20(Batch%20normalization)&text=La%20normalizaci%C3%B3n%20en%20lotes%20consiste,normalizar%20las%20activaciones%20de%20salida.\n",
        "* https://medium.com/@bdhuma/which-pooling-method-is-better-maxpooling-vs-minpooling-vs-average-pooling-95fb03f45a9"
      ]
    }
  ]
}